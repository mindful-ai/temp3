{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2456164-3494-464e-bc05-d1b75a4b6ec3",
   "metadata": {},
   "source": [
    "### 1.1 Prior Installations and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0cabca-0390-498b-a9f5-6f6acc4bb822",
   "metadata": {},
   "source": [
    "pip install openai\n",
    "pip install langchain\n",
    "pip install langchain-openai\n",
    "pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc63917e-739b-484b-b655-e5f822a4839b",
   "metadata": {},
   "source": [
    "Register for a API key @ https://platform.openai.com/\n",
    "Your Profile -> User API Keys\n",
    "Copy the key to a text file. \n",
    "Make sure it's kept secure\n",
    "Have some $5 credits as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac44fc6c-c365-43a9-a2f6-48e7c6aae5f5",
   "metadata": {},
   "source": [
    "### 1.2 Basic Invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a522fccc-b87b-457c-ab13-e9babd3f9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "f = open(r\"C:\\Users\\mindf\\Desktop\\current-work\\openai-api-key-purushotham.txt\")\n",
    "apikey = f.read()\n",
    "f.close()\n",
    "\n",
    "llm = OpenAI(api_key=apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8f30a-112c-4962-8b66-ce7e198a7203",
   "metadata": {},
   "source": [
    "#### A simple way to get text auto complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35b156-802f-4688-85e9-ff4fe37be73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke('Here is a fun fact about Pluto:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9caa095-f3e8-4a2f-974b-9fc9c09244cc",
   "metadata": {},
   "source": [
    "#### Use generate for full output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c376f98-fd37-4ccd-bcba-d66fe0fd49e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEDS TO BE A LIST, EVEN FOR JUST ONE STRING\n",
    "result = llm.generate(['Here is a fun fact about Pluto:',\n",
    "                     'Here is a fun fact about Mars:']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9a38a-52f1-402f-ae98-4839959be59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01418c-2c5f-499a-a33d-418738805f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5ecdd2-d51e-464f-a3be-6b183e833b24",
   "metadata": {},
   "source": [
    "### 1.3 Chat Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72533584-3046-4233-a110-43418aefe54b",
   "metadata": {},
   "source": [
    "The most popular models are actually chat models, that have a System Message and then a series of Assistant and Human Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52374d70-7df7-43de-8857-bf1532fd883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(openai_api_key=apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af47fa6-4068-4601-b9d1-c964bd9547f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe551f4e-543d-46bb-9a26-9a7ada1f0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke([HumanMessage(content=\"Can you tell me a fact about Earth?\")])\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c1bf4-db7f-4272-9ebd-19525daadb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke([SystemMessage(content='You are a very rude teenager who only wants to party and not answer questions'),\n",
    "               HumanMessage(content='Can you tell me a fact about Earth?')])\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32122296-a234-4004-8df4-6028a62086fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEDS TO BE A LIST!\n",
    "result = chat.generate(\n",
    "                [\n",
    "                    [  SystemMessage(content='You are a University Professor'),\n",
    "                       HumanMessage(content='Can you tell me a fact about Earth?') ]\n",
    "                ]\n",
    ")\n",
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece3a5c-5876-4c70-99cd-909b123cdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.generations[0][0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c583070e-0ebc-457d-ad67-c6a3466c71e0",
   "metadata": {},
   "source": [
    "### 1.4 Extra Paramters and Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae00989-cea2-44d2-90d1-7335953a8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke([HumanMessage(content='Can you tell me a fact about Earth?')],\n",
    "                 temperature=2,presence_penalty=1,max_tokens=100)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543543af-043f-4c07-adad-6cc8bc58c292",
   "metadata": {},
   "source": [
    "### 2.0 Understanding Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f2948b-d2b4-42e3-bc67-df95da65b36c",
   "metadata": {},
   "source": [
    "#### 2.1 Input Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff57ab-11bd-477b-8b53-0903f8357be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# An example prompt with no input variables\n",
    "no_input_prompt = PromptTemplate(input_variables=[], template=\"Tell me a fact\")\n",
    "no_input_prompt.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7524b143-e706-4549-8b9e-31ea048ed927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example prompt with one input variable\n",
    "one_input_prompt = PromptTemplate(input_variables=[\"topic\"], template=\"Tell me a fact about {topic}.\")\n",
    "# Notice how the stirng \"topic\" gets automatically converted to a parameter name, very convienent! \n",
    "one_input_prompt.format(topic=\"Mars\")\n",
    "# -> \"Tell me a fact about Mars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c993f3-7aea-4983-a667-3a0092ed9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example prompt with multiple input variables\n",
    "multiple_input_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"level\"], \n",
    "    template=\"Tell me a fact about {topic} for a student {level} level.\"\n",
    ")\n",
    "multiple_input_prompt.format(topic='Mars',level='8th Grade')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7572536-ccae-4ff6-bf55-6ee4cd1d6ba0",
   "metadata": {},
   "source": [
    "#### 2.2 Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b574670-6cb3-4bfd-af84-7f387ff003cf",
   "metadata": {},
   "source": [
    "Chat models require a list of chat messages called a prompt, which is different from a raw string that you would input into a language model. Each message in the prompt is associated with a role, such as AI, human, or system.\n",
    "\n",
    "For instance, when using the OpenAI Chat Completion API, a chat message can be assigned the role of AI, human, or system. The model is designed to pay closer attention to instructions provided in system chat messages.\n",
    "\n",
    "To simplify the process of constructing and working with prompts, LangChain offers various prompt templates. It is highly recommended to utilize these chat-related prompt templates instead of PromptTemplate when interacting with chat models. This will allow you to fully harness the potential of the underlying chat model and enhance your experience.\n",
    "\n",
    "We will favor these models in the course due to upcoming changes in the OpenAI ecosystem where chat agents will be favored over text completion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa57da8-c686-4991-a7d2-6afe77769186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2d7db-a0ed-4744-9414-7871815773cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template=\"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ac90c-eee1-4802-b5ff-ec6217a9f80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa57840f-938f-47f3-8163-58cbe7ae1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template=\"{recipe_request}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc74c2-3ec2-4efe-a635-6c136971f979",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfe9fe-b98b-49d2-9fec-648ae8da9a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620828ca-422d-426f-a29a-e66e662773a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a7656-5f31-4835-ab6d-ac8e5352be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a chat completion from the formatted messages\n",
    "chat_prompt.format_prompt(cooking_time=\"15 min\", dietary_preference=\"Vegan\", recipe_request=\"Quick Snack\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144842d-67a3-4543-8adc-94035f5461f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = chat_prompt.format_prompt(cooking_time=\"15 min\", dietary_preference=\"Vegan\", recipe_request=\"Quick Snack\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cc31e-f2fb-4149-ac79-a81bb997b7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d0770-1baa-4c91-8dce-92c55767d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4fad53-8852-4cae-9e16-e4ca36a3773c",
   "metadata": {},
   "source": [
    "#### 2.3 Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65789e-7440-40a2-ab44-ecad93b6ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant that translates complex legal terms into plain and understandable terms.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03c0b8-0379-492f-9f96-42404551ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_text = \"The provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, illegal, or unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall remain in full force and effect to the maximum extent permitted by law.\"\n",
    "example_input_one = HumanMessagePromptTemplate.from_template(legal_text)\n",
    "\n",
    "# Use this for creating example AI prompt\n",
    "plain_text = \"The rules in this agreement can be separated. If a court decides that one rule or part of it is not valid, illegal, or cannot be enforced, the other rules will still apply and be enforced as much as they can under the law.\"\n",
    "example_output_one = AIMessagePromptTemplate.from_template(plain_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f065d-1607-4dba-8a7c-d9ebc84fd33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"{legal_text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc192bb-8ba8-4548-ad16-18c152dd6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [system_message_prompt, example_input_one, example_output_one, human_message_prompt]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6613ed-7139-4dd3-968d-53f82e2620ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"The grantor, being the fee simple owner of the real property herein described, conveys and warrants to the grantee, his heirs and assigns, all of the grantor's right, title, and interest in and to the said property, subject to all existing encumbrances, liens, and easements, as recorded in the official records of the county, and any applicable covenants, conditions, and restrictions affecting the property, in consideration of the sum of [purchase price] paid by the grantee.\"\n",
    "request = chat_prompt.format_prompt(legal_text=example_text).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1b866-db9f-41aa-8b19-959b363b7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ada69-8db0-4fc9-aef7-c21a83abd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e340c6a5-5048-419b-bff5-f3e5039813eb",
   "metadata": {},
   "source": [
    "### 3.0 Exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163aa086-bf1f-4299-b584-5640768f6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from datetime import datetime\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b5efa-08d2-4a61-b034-c563f5b92650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryQuiz():\n",
    "    \n",
    "    def create_history_question(self,topic):\n",
    "        '''\n",
    "        This method should output a historical question about the topic that has a date as the correct answer.\n",
    "        For example:\n",
    "        \n",
    "            \"On what date did World War 2 end?\"\n",
    "            \n",
    "        '''\n",
    "       \n",
    "        return question\n",
    "    \n",
    "    def get_AI_answer(self,question):\n",
    "        '''\n",
    "        This method should get the answer to the historical question from the method above.\n",
    "        Note: This answer must be in datetime format! Use DateTimeOutputParser to confirm!\n",
    "        \n",
    "        September 2, 1945 --> datetime.datetime(1945, 9, 2, 0, 0)\n",
    "        '''\n",
    "         \n",
    "        \n",
    "        return correct_datetime\n",
    "    \n",
    "    def get_user_answer(self,question):\n",
    "        '''\n",
    "        This method should grab a user answer and convert it to datetime. It should collect a Year, Month, and Day.\n",
    "        You can just use input() for this.\n",
    "        '''\n",
    "        \n",
    "\n",
    "        \n",
    "        return user_datetime\n",
    "        \n",
    "        \n",
    "    def check_user_answer(self,user_answer,ai_answer):\n",
    "        '''\n",
    "        Should check the user answer against the AI answer and return the difference between them\n",
    "        '''\n",
    "        # print or return the difference between the answers here!\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ed1ff-44d2-4c99-8ce1-0742adbf6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_bot = HistoryQuiz()\n",
    "question = quiz_bot.create_history_question(topic='World War 2')\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07129c76-c31b-45e1-a8ca-845b01a38a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_answer = quiz_bot.get_AI_answer(question)\n",
    "ai_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25bc41d-8adc-4f0f-a44b-f16be9f527bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_answer = quiz_bot.get_user_answer(question)\n",
    "user_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da3f5c-3610-436e-83c7-d7be12578006",
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_bot.check_user_answer(user_answer,ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192de1f-2bea-4528-8a2b-8ffd07e7cede",
   "metadata": {},
   "source": [
    "### 4.0 Document Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9dc40-3a8e-4c4f-a57e-6f3d80165e41",
   "metadata": {},
   "source": [
    "There are many other types of Documents that can be loaded in. You can see all the document loaders available here: https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "\n",
    "Keep in mind many Loaders are dependent on other libraries, meaning issues in those libraries can end up breaking the Langchain loaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4e86e-d508-4709-9cc8-ac505379de67",
   "metadata": {},
   "source": [
    "#### 4.1 CSV Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e5ecd-32f9-4e77-ac84-e10ccceab013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d50199-371c-4335-bca4-05811e92b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain\\penguins.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8734e6-ffe9-422c-a9b5-508542761559",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b223f-4445-4036-8909-f8119427f05e",
   "metadata": {},
   "source": [
    "#### 4.2 HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d4aad-8454-4885-b0e4-11e293b64f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import BSHTMLLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5fec1-ce0b-4f56-a3c2-25127f63d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = BSHTMLLoader(r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain\\some_website.html')\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf4573-7ba4-4db9-8ab3-9df4bc030e3d",
   "metadata": {},
   "source": [
    "#### 4.3 PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08cb9b-6cfb-4aee-81dc-dfc8dbfe67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6662f28-9994-4c2e-811a-f4173199ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9577189f-2aaa-4804-a487-619e2b920e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain\\report.pdf')\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25b72e-99d8-44bf-8eac-3894a6fd77af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01171de-cd96-4f91-8ae6-c0981f097b85",
   "metadata": {},
   "source": [
    "#### 4.4 Document Tranformations: Split by Character, split by tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b09c825-ef60-4d78-815a-80f5502e2a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain\\FDR_State_of_Union_1944.txt') as file:\n",
    "    speech_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f87668-0753-4e26-a79e-aa5a17a9956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9e88a-2982-4e1f-ac20-4942b63ab868",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\",chunk_size=1000) #1000 is default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef4ebc-fc83-4657-849d-5062a4702b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([speech_text])\n",
    "print(type(texts))\n",
    "print('\\n')\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237cad69-7605-40bb-a163-9a1ba0002191",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366b497-63b7-4736-8855-71c2b41cfae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977cba1-706e-4174-83d8-f1974b033925",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500) #now chunk size is a hard length based on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75d3ea-837e-49c3-a7af-9ee92779b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(speech_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d152f4-440d-4e5e-9e3a-5dc693ea9dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a86037-b31b-4dff-a1f6-84dba769240c",
   "metadata": {},
   "source": [
    "#### 4.5 Text Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c2955-7f66-476a-a179-86f0c7fe9c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89cec33-28e7-48f9-9e79-6b635c243d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key=apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947efba4-8dd7-444e-b0e1-80a1789582c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Some normal text to send to OpenAI to be embedded into a N dimensional vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bc00c-f861-461b-bec2-70b28db70d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1721dc0-39d8-4ed0-a6a2-102e8da9fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167cf237-524f-467e-b243-46e784d39e14",
   "metadata": {},
   "source": [
    "### 5.0 Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a889ed-5028-4504-8c53-6d577445bc71",
   "metadata": {},
   "source": [
    "##### We can save the embeddings into a Vector store - Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded29a54-05ae-4265-9394-32012a232400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc46a57-fe27-4971-b485-80445b6f725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "print(chromadb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9f5a6-d855-4a70-918e-1c3de449b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483567f-0fe8-4ef8-a7b4-a134ec7ae439",
   "metadata": {},
   "source": [
    "##### Load the document and split(still recommended even if under the context window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d5af4-d42a-49a2-af4c-61196dbd9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the document and split it into chunks\n",
    "loader = TextLoader(r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain\\FDR_State_of_Union_1944.txt')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df2597-f677-47db-bcde-ab53c477fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17f03d-d95d-4381-b120-0a611eadd930",
   "metadata": {},
   "source": [
    "##### Connect to OpenAI for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8831b39-212d-43c4-9480-bbea55820697",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = OpenAIEmbeddings(api_key=apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec87ba5-6140-460c-bbfe-ab5e96321935",
   "metadata": {},
   "source": [
    "##### Pass Embeddings and Docs into Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908569f9-d549-4178-bede-1f8d0926815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function,persist_directory=r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain-course\\speech_embedding_db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b246089-0629-4523-a2db-96a9757a91e5",
   "metadata": {},
   "source": [
    "##### Save the new embeddings to the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7a675-97f6-4d8e-88ab-4c2b6ac0391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful to force a save\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e7bdb-06a7-4fcf-b4cb-07298ae0ab62",
   "metadata": {},
   "source": [
    "##### Loading embeddings from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc45cf-237c-495b-9246-2b2975883dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory=r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain-course\\speech_embedding_db'\n",
    "db_connection = Chroma(persist_directory=persist_directory, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da68e69-81fc-4a22-a2d1-bacd92c35291",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"What did FDR say about the cost of food law?\"\n",
    "docs = db_connection.similarity_search(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6989a9c0-34be-4d42-a4b8-7a513f5ab550",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3c017-de75-479b-b79f-ee74e4f7b31e",
   "metadata": {},
   "source": [
    "##### Adding a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab5f62-64ee-4091-b1ab-fe53c73ca294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the document and split it into chunks\n",
    "loader = TextLoader(r\"C:\\Users\\mindf\\Desktop\\sapient-us\\langchain-course\\Lincoln_State_of_Union_1862.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ab981-d092-453f-8d44-df05f02c765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8864422-1284-45bf-9356-e8f7542ba4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embedding_function,persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc7de7-81c8-44b1-9ccb-7bb20c367a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search('slavery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f4a1ed-1ee4-43dd-976a-2f96298d6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601641af-e816-4b49-9e4a-76301c2a3cbf",
   "metadata": {},
   "source": [
    "### 5.1 Vector Store Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7ab412-b057-49f7-b534-1e0104d3fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6e96c-d966-47a2-93d5-314653d53b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_function = OpenAIEmbeddings(api_key=apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5759efb-bdc3-4d88-a7c3-a078c407a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = Chroma(persist_directory=r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain-course\\mk_ultra',embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f77106-b76f-4428-b9d0-f8779c74511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_connection.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111df4b0-63f7-4b2d-a634-e5d62d69239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_kwargs = {\"score_threshold\":0.8,\"k\":4}\n",
    "docs = retriever.invoke(\"President\",search_kwargs=search_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485d1fd-aaf6-4691-b5da-b5386fc22d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c20831-a7c7-45de-b158-bae3bb57aa3d",
   "metadata": {},
   "source": [
    "### 6.0 Exercise : Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74e4b8f-b3b8-4019-b7d5-d5e2bebadb42",
   "metadata": {},
   "source": [
    "###  Data Connections Exercise\n",
    "\n",
    "#### Ask a Legal Research Assistant Bot about the US Constitution\n",
    "\n",
    "Let's revisit our first exercise and add offline capability using ChromaDB. Your function should do the following:\n",
    "\n",
    "* Read the US_Constitution.txt file inside the some_data folder\n",
    "* Split this into chunks (you choose the size)\n",
    "* Write this to a ChromaDB Vector Store\n",
    "* Use Context Compression to return the relevant portion of the document to the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896a090-194d-451f-a1f5-67d38ec25139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab2712-5658-4eb2-b020-cc42f2a7a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def us_constitution_helper(question):\n",
    "    '''\n",
    "    Takes in a question about the US Constitution and returns the most relevant\n",
    "    part of the constitution. Notice it may not directly answer the actual question!\n",
    "    \n",
    "    Follow the steps below to fill out this function:\n",
    "    '''\n",
    "\n",
    "    persist_directory=r\"C:\\Users\\mindf\\Desktop\\sapient-us\\langchain-course\"\n",
    "    \n",
    "    # PART ONE:\n",
    "    # LOAD \"/US_Constitution in a Document object\n",
    "    loader = TextLoader(r\"C:\\Users\\mindf\\Desktop\\sapient-us\\langchain-course\\US_Constitution.txt\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # PART TWO\n",
    "    # Split the document into chunks (you choose how and what size)\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=500)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # PART THREE\n",
    "    # EMBED THE Documents (now in chunks) to a persisted ChromaDB\n",
    "    embedding_function = OpenAIEmbeddings(api_key=apikey)\n",
    "    db = Chroma.from_documents(docs, embedding_function,persist_directory=persist_directory)\n",
    "    db.persist()\n",
    "\n",
    "    # PART FOUR\n",
    "    # Use ChatOpenAI and ContextualCompressionRetriever to return the most\n",
    "    # relevant part of the documents.\n",
    "\n",
    "    # results = db.similarity_search(\"What is the 13th Amendment?\")\n",
    "    # print(results[0].page_content) # NEED TO COMPRESS THESE RESULTS!\n",
    "    llm = ChatOpenAI(temperature=0, api_key=apikey)\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "    compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, \n",
    "                                                           base_retriever=db.as_retriever())\n",
    "\n",
    "    compressed_docs = compression_retriever.invoke(question)\n",
    "\n",
    "    return compressed_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51b75b-c5bb-4787-ac1e-0c04ea3651fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(us_constitution_helper(\"What is the 13th Amendment?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa3deb-b919-45ad-8fa4-f366bf92cbba",
   "metadata": {},
   "source": [
    "### 7.0 Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5852ccd3-d3dc-4102-9060-2a21bbf1ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.sequential import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372e1c5-ea7b-4ad4-b362-bf5717dd4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(api_key=apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d7148-efdd-47c3-8a21-8b9113d3fab6",
   "metadata": {},
   "source": [
    "##### Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e20a3-7619-4834-b5a1-f5cc856ac174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76774b9-8349-4a7b-af98-388ac7e25294",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\n",
    "        \"Make up a funny company name for a company that produces {product}\"\n",
    "    )\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ddeb84-d856-4e4b-b291-b6e14cd4838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d131d5-7122-49d8-b711-6951271b9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke(input=\"Computers\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68f709-3b24-4d3b-9aa9-69c84ae6a17e",
   "metadata": {},
   "source": [
    "##### Sequntial Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5ff5f-cc5c-4c04-82bc-69b216e3631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = \"Give a summary of this employee's performance review:\\n{review}\"\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "chain_1 = prompt1|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8268e-fef3-4260-b53a-d967309373de",
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = \"Identify key employee weaknesses in this review summary:\\n{review_summary}\"\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "chain_2 = prompt2|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89f54e-0b96-455b-adff-9d3743bf8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "template3 = \"Create a personalized plan to help address and fix these weaknesses:\\n{weaknesses}\"\n",
    "prompt3 = ChatPromptTemplate.from_template(template3)\n",
    "chain_3 = prompt3|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee01d7-220f-45db-8e82-cbbe4b1b3869",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = chain_1|chain_2|chain_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e0b78-7638-45b8-aa76-8975c804c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_review = '''\n",
    "Employee Information:\n",
    "Name: Joe Schmo\n",
    "Position: Software Engineer\n",
    "Date of Review: July 14, 2023\n",
    "\n",
    "Strengths:\n",
    "Joe is a highly skilled software engineer with a deep understanding of programming languages, algorithms, and software development best practices. His technical expertise shines through in his ability to efficiently solve complex problems and deliver high-quality code.\n",
    "\n",
    "One of Joe's greatest strengths is his collaborative nature. He actively engages with cross-functional teams, contributing valuable insights and seeking input from others. His open-mindedness and willingness to learn from colleagues make him a true team player.\n",
    "\n",
    "Joe consistently demonstrates initiative and self-motivation. He takes the lead in seeking out new projects and challenges, and his proactive attitude has led to significant improvements in existing processes and systems. His dedication to self-improvement and growth is commendable.\n",
    "\n",
    "Another notable strength is Joe's adaptability. He has shown great flexibility in handling changing project requirements and learning new technologies. This adaptability allows him to seamlessly transition between different projects and tasks, making him a valuable asset to the team.\n",
    "\n",
    "Joe's problem-solving skills are exceptional. He approaches issues with a logical mindset and consistently finds effective solutions, often thinking outside the box. His ability to break down complex problems into manageable parts is key to his success in resolving issues efficiently.\n",
    "\n",
    "Weaknesses:\n",
    "While Joe possesses numerous strengths, there are a few areas where he could benefit from improvement. One such area is time management. Occasionally, Joe struggles with effectively managing his time, resulting in missed deadlines or the need for additional support to complete tasks on time. Developing better prioritization and time management techniques would greatly enhance his efficiency.\n",
    "\n",
    "Another area for improvement is Joe's written communication skills. While he communicates well verbally, there have been instances where his written documentation lacked clarity, leading to confusion among team members. Focusing on enhancing his written communication abilities will help him effectively convey ideas and instructions.\n",
    "\n",
    "Additionally, Joe tends to take on too many responsibilities and hesitates to delegate tasks to others. This can result in an excessive workload and potential burnout. Encouraging him to delegate tasks appropriately will not only alleviate his own workload but also foster a more balanced and productive team environment.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49927a-040a-4afa-92e7-a198be0d4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = seq_chain.invoke(employee_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0bcc5-064f-4f74-99cd-4056b8237fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017b68c-1c71-4d37-9ac1-d7627db6b12d",
   "metadata": {},
   "source": [
    "### 8.0 Exercise - Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821359c5-c7da-44c1-97ac-ec4521b8645e",
   "metadata": {},
   "source": [
    "####  Chains Exercise - Solution\n",
    "\n",
    "#### TASK:\n",
    "Fill out the function below that takes in a string input Customer Support email that could be written in any language. The function will then detect the language, translate the email, and provide a summary.\n",
    "\n",
    "Fill out the function below using a Sequential Chain, the function should do the following:\n",
    "\n",
    "1. Detect the language the email is written in\n",
    "2. Translate the email from detected language to English\n",
    "3. Return a summary of the translated email\n",
    "\n",
    "Note: The Function should return a dictionary that contains all three of these outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f3264-eace-4fa6-ba0d-369401e00595",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_email = open(r'C:\\Users\\mindf\\Desktop\\sapient-us\\langchain-course\\spanish_customer_email.txt', encoding=\"latin-1\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf754ed-83c0-4a7e-a64d-d3f170480f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spanish_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4643e1-5222-4daa-9731-4ced086be6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f91c08-071f-48ff-a4be-3dae35d1af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_and_summarize(email):\n",
    "    \"\"\"\n",
    "    Translates an email written in a detected language to English and generates a summary.\n",
    "\n",
    "    Args:\n",
    "        email (str): The email to be processed and translated.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - 'language': The language the email was written in.\n",
    "            - 'translated_email': The translated version of the email in English.\n",
    "            - 'summary': A short summary of the translated email.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs during the LLM chain execution.\n",
    "\n",
    "    Example:\n",
    "        email = \"Hola, ¿cómo estás? Espero que todo vaya bien.\"\n",
    "        result = translate_and_summarize(email)\n",
    "        print(result)\n",
    "        # Output:\n",
    "        # {\n",
    "        #     'language': 'Spanish',\n",
    "        #     'translated_email': 'Hello, how are you? I hope everything is going well.',\n",
    "        #     'summary': 'A friendly greeting and a wish for well-being.'\n",
    "        # }\n",
    "    \"\"\"\n",
    "    # Create Model\n",
    "    llm = ChatOpenAI(api_key=apikey)\n",
    "    \n",
    "    # CREATE A CHAIN THAT DOES THE FOLLOWING:\n",
    "    \n",
    "    # Detect Language\n",
    "    template1 = \"Return the language this email is written in:\\n{email}.\\nONLY return the language it was written in.\"\n",
    "    prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "    chain_1 = prompt1|llm\n",
    "    \n",
    "    # Translate from detected language to English\n",
    "    template2 = \"Translate this email from {language} to English. Here is the email:\\n\"+email\n",
    "    prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "    chain_2 = prompt2|llm\n",
    "    \n",
    "    # Return English Summary AND the Translated Email\n",
    "    template3 = \"Create a short summary of this email:\\n{translated_email}\"\n",
    "    prompt3 = ChatPromptTemplate.from_template(template3)\n",
    "    chain_3 = prompt3|llm\n",
    "\n",
    "    language_chain = chain_1\n",
    "    translation_chain = language_chain|chain_2\n",
    "    seq_chain = translation_chain|chain_3\n",
    "    \n",
    "    return language_chain.invoke(email), translation_chain.invoke(email), seq_chain.invoke(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ca8bb-5520-4bfe-92d1-ce1fec957b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = translate_and_summarize(spanish_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681b9c22-a10a-443f-a106-33cbff4aa08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c967dd-79bb-4c7e-bf85-90eb0946e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6479ccd-a955-4ed1-83e3-a9b9ee19213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af1a51-ef1e-47af-884a-5113f80874a3",
   "metadata": {},
   "source": [
    "### 9.0 Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4944b8-035f-4ec2-b377-80427332f658",
   "metadata": {},
   "source": [
    "##### Chat Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ed264-31e3-4a2f-891e-a6b085f4d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Hello, nice to meet you.\")\n",
    "history.add_ai_message(\"Nice to meet you too!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b222dbd-cf17-4f11-8c50-9b4f05e0e784",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005cdff2-f805-4ea3-9514-b8fd3928e700",
   "metadata": {},
   "source": [
    "##### Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebeb63e-2777-4c34-858d-fd8b8479a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315b267-0795-41e3-b013-dfe46de1d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, api_key=apikey)\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f413c9d2-eba7-4b0b-a5bd-464c4553d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9403b4a-f6b7-446b-81e4-f32ce57aa698",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke(input=\"Hello, nice to meet you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d4c36-6f36-4d39-b333-4b49c144f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke(input=\"Tell me about the Einstein-Szilard Letter \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7777327-a027-4abe-a0fc-69810eac7fa6",
   "metadata": {},
   "source": [
    "##### Saving and Loading Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7036b-c679-41bc-aa8f-7e18d180e8ab",
   "metadata": {},
   "source": [
    "Best Source We've Found: https://stackoverflow.com/questions/75965605/how-to-persist-langchain-conversation-memory-save-and-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102140b-b3be-422f-92fc-fb3fc5067fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cb63e-1930-4074-aea0-6f7f73dd2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickled_str = pickle.dumps(conversation.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de984e-7eb2-4856-bb30-df5a2fc9aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('memory.pkl','wb') as f:\n",
    "    f.write(pickled_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3956aa-2886-4bad-a4c0-e6e29000e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_memory_load = open('memory.pkl','rb').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0fe3b-32b7-401c-8c86-c266b97218d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, api_key=apikey)\n",
    "reload_conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = pickle.loads(new_memory_load),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57932f2f-072e-4da8-97ea-3e92789ea5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_conversation.memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d4a96-dc3c-4a06-9ecd-1018df4bb2c3",
   "metadata": {},
   "source": [
    "### 10.0 Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f296c1f9-2a93-41de-a81e-434db4d16d8d",
   "metadata": {},
   "source": [
    "The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
    "\n",
    "This is the chain responsible for deciding what step to take next. This is usually powered by a language model, a prompt, and an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3b76d-f60b-490c-bf91-c78c0f406aeb",
   "metadata": {},
   "source": [
    "There are several key concepts to understand when building agents: Agents, AgentExecutor, Tools, Toolkits. For an in depth explanation, please check out this conceptual guide: https://python.langchain.com/v0.1/docs/modules/agents/concepts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77150cd7-19df-45de-966f-1a6632d94838",
   "metadata": {},
   "source": [
    "##### Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f4f373e2-b1a4-44d0-a43b-4b78b0d994fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96df7e4-800e-46ce-97aa-edda46666fef",
   "metadata": {},
   "source": [
    "##### Define Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905142d5-8d38-4e30-8d93-639a3a9d5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Returns the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "get_word_length.invoke(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "12f06424-a57b-42d0-9037-ae5abf8390b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_word_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092d5f7-bd65-468e-b9fb-4d30d0356238",
   "metadata": {},
   "source": [
    "##### Create a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "69d1a49f-f490-461c-8a69-17c9aa8185c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but don't know current events\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd244d-8c27-4153-8bde-6c4a5ec0df59",
   "metadata": {},
   "source": [
    "##### Bind tools to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c7acd131-7a46-4a65-be64-dd435ed45a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58e621-f467-4b95-93cb-11f316e7bbf4",
   "metadata": {},
   "source": [
    "##### Create the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9fa5ff-bb3e-442c-a70c-586b525d98a4",
   "metadata": {},
   "source": [
    "Putting those pieces together, we can now create the agent. We will import two last utility functions: a component for formatting intermediate steps (agent action, tool output pairs) to input messages that can be sent to the model, and a component for converting the output message into an agent action/agent finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9b8152a0-845b-48c2-91d2-f3ec005892c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f8a757f4-226e-478d-aa90-d223abba3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae796f7-469e-456b-b305-3742af1ca428",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(agent_executor.stream({\"input\": \"How many letters in the word purushotham\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f53b31a-3d18-4677-abe3-08cedee03ad8",
   "metadata": {},
   "source": [
    "##### If we compare this to base LLM, we can see that LLM alone struggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d614639-5f73-44a2-9e4a-d39ca5c191d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"How many letters in the word purushotham\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7969aa-2533-46bc-9f83-404b63883d0b",
   "metadata": {},
   "source": [
    "### 11. Simple Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3ee1f-48eb-4d68-bd06-144348a157d7",
   "metadata": {},
   "source": [
    "Imagine you are just starting out with an ice-cream business and want to learn everything about ice-creams (your ideal customers, unique flavor combinations, easy ice-cream recipes etc.). Let’s call our ice-cream assistant Scoopsie. We’ll develop our chatbot using LangChain and OpenAI’s text completion model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a45c54-9126-4b1b-a39e-f80447b3f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0.7, api_key=apikey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9e47a-02f5-445b-92a7-389b81f8e2b0",
   "metadata": {},
   "source": [
    "Next, we need to create a LLM chain. In LangChain, LLM chains represent a higher-level abstraction for interacting with language models. While we can use the direct LLM interface in our simple chatbot, the LLMChain interface wraps an LLM to add additional functionality. With chains, we can have prompt formatting and input/output parsing, and they are used extensively by higher level LangChain tools. For our simple chatbot, we will use the LLMChain, and pass it the model object we created, alongwith our ice-cream assistant prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b3bd117-fd4f-49d9-8028-b370d3a4cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "ice_cream_assistant_template = \"\"\"\n",
    "You are an ice cream assistant chatbot named \"Scoopsie\". Your expertise is \n",
    "exclusively in providing information and advice about anything related to ice creams. This includes flavor combinations, ice cream recipes, and general \n",
    "ice cream-related queries. You do not provide information outside of this \n",
    "scope. If a question is not about ice cream, respond with, \"I specialize only in ice cream related queries.\" \n",
    "Question: {question} \n",
    "Answer:\"\"\"\n",
    "\n",
    "ice_cream_assistant_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=ice_cream_assistant_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62808cb7-de73-4b04-87f7-8981d76fe491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=ice_cream_assistant_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd69a6e-1633-49fb-a735-fd70696644fe",
   "metadata": {},
   "source": [
    "Our simple chatbot’s code is mostly complete. We just need to create an entry point for our script aka the main function. We also need to be able to query our model. This is done using the invoke function with the llm_chain object. Putting together everything we have done so far in our chatbot.py script, you will have something that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a52bed-7a66-4569-a57c-a4edce84c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=ice_cream_assistant_prompt_template)\n",
    "\n",
    "\n",
    "def query_llm(question):\n",
    "    print(llm_chain.invoke({'question': question})['text'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    query_llm(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1326b62a-4899-4076-9021-ca4e1e24136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I need chocolate ice-cream recipe\"\n",
    "print(llm_chain.invoke({'question': question})['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0451f7f-5f8f-4bab-b379-013f463d8113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
